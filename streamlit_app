import streamlit as st
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Page Config
st.set_page_config(page_title="Country Development Clustering", layout="wide")

# Title and Introduction
st.title("Country Development Clustering")
st.write("""
This application clusters countries into three categories: Developed, **Developing, and **Underdeveloped 
based on key development metrics. The clustering model helps to group countries for analysis and decision-making.
""")

# Sidebar for file upload
st.sidebar.header("Upload Dataset")
uploaded_file = st.sidebar.file_uploader("Choose an Excel file", type="xlsx")

# Clean data function to handle percentage and currency values
def clean_data(df):
    for column in df.columns:
        # Only process columns with string data
        if df[column].dtype == 'object':
            # Remove '%' and convert to float for percentage columns
            if df[column].str.contains('%').any():
                df[column] = df[column].str.replace('%', '').astype(float) / 100.0
            # Remove ',' and '$' symbols from currency columns and convert to float
            elif df[column].str.contains(r'[,\$]').any():
                df[column] = df[column].str.replace('[\$,]', '', regex=True).astype(float)
    return df

# Load Data and Preprocess
if uploaded_file is not None:
    data = pd.read_excel(uploaded_file)

    # Display the raw data
    st.subheader("Dataset Overview")
    st.write("### Raw Data")
    st.write(data.head())

    # Select and clean features for clustering
    numeric_features = data.select_dtypes(include=["float64", "int64"]).columns.tolist()
    selected_features = st.multiselect("Select Features for Clustering", numeric_features, default=numeric_features)
    
    if selected_features:
        # Select only the numeric columns chosen by the user for clustering
        data_selected = data[selected_features]

        # Clean and preprocess selected features
        data_selected = clean_data(data_selected)

        # Impute missing values with median and standardize the data
        data_selected = data_selected.fillna(data_selected.median())
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_selected)

        # Determine Optimal Number of Clusters using Elbow Method
        st.write("### Elbow Method for Optimal Cluster Count")
        inertia = []
        K = range(1, 11)
        for k in K:
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(data_scaled)
            inertia.append(kmeans.inertia_)
        
        # Plot Elbow Method
        fig, ax = plt.subplots()
        ax.plot(K, inertia, 'bo-')
        ax.set_xlabel("Number of Clusters")
        ax.set_ylabel("Inertia")
        ax.set_title("Elbow Method for Optimal k")
        st.pyplot(fig)
        
        # Cluster the data using the optimal k
        optimal_k = st.slider("Select Number of Clusters (k)", 2, 10, 3)
        kmeans = KMeans(n_clusters=optimal_k, random_state=42)
        clusters = kmeans.fit_predict(data_scaled)
        data["Cluster"] = clusters

        # Evaluate Model
        silhouette_avg = silhouette_score(data_scaled, clusters)
        calinski_harabasz = calinski_harabasz_score(data_scaled, clusters)
        st.write(f"Silhouette Score: {silhouette_avg:.2f}")
        st.write(f"Calinski-Harabasz Score: {calinski_harabasz:.2f}")

        # Display Clustered Data
        st.subheader("Clustered Data with Labels")
        st.write(data.head())

        # Rename clusters for interpretability
        cluster_names = {0: "Underdeveloped", 1: "Developing", 2: "Developed"}
        data["Cluster Label"] = data["Cluster"].map(cluster_names)
        
        # PCA for Visualization
        n_samples, n_features = data_scaled.shape
        pca_components = min(n_samples, n_features, 2)
        
        if pca_components > 1:
            pca = PCA(n_components=pca_components)
            data_pca = pca.fit_transform(data_scaled)
            data["PCA1"] = data_pca[:, 0]
            data["PCA2"] = data_pca[:, 1]

            # Plot Clusters
            st.write("### Cluster Visualization (PCA Reduced)")
            fig, ax = plt.subplots()
            sns.scatterplot(data=data, x="PCA1", y="PCA2", hue="Cluster Label", palette="viridis", ax=ax)
            ax.set_title("PCA Plot of Clusters")
            st.pyplot(fig)
        else:
            st.warning("Not enough samples or features for PCA visualization.")

        # Decision-making suggestions based on clusters
        st.subheader("Decision-Making Suggestions")
        st.write("""
        - Developed Countries: These countries have strong economic indicators. Investments or partnerships can be considered.
        - Developing Countries: These countries are on the path to growth. Development aid and infrastructure projects can be beneficial.
        - Underdeveloped Countries: These countries need more significant support. Consider humanitarian aid, education, and healthcare projects.
        """)

        # Download Clustered Data
        def convert_df(df):
            return df.to_csv(index=False).encode('utf-8')

        csv = convert_df(data)
        st.download_button(
            label="Download Clustered Data as CSV",
            data=csv,
            file_name='clustered_countries.csv',
            mime='text/csv',
        )
    else:
        st.warning("Please select at least one feature for clustering.")
else:
    st.info("Please upload an Excel file to proceed.")
